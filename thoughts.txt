The Wiki Game is tough. Would be cool to write a bot that could play it,
compete against humans, and hopefully win some of the time.

A) Approach:

1. Webscrape the body of the current wiki page and extract all the hyper-
   links
2. Analyse the title of each link's page and give it a score based on how 
   close that title is to the target page's title (see section B)
3. Track the index of the max score, and load up the page once all links have
   been analysed. 
4. If it is the traget page, bot wins. Else, repeat steps 1 onwards.

B) Link Analysing

Most sensible approach would likely involve some form of machine learning with
a convolutional neural network for NLP, but I lack the time and expertise for
something that complex. 

I'm going to opt for something much simpler instead. The algorithm for link
analysis would look something akin to this:

For each url:
1. Strip/trim the url so we have just the title of the page (no caps)
2. Compare the 'difference' (diff) between that title and the target title:
	i) What's the difference in length?
	ii) how many of the same chars are there? (bonus if they're in the 
	    same spot)
	iii) Sum everything and deliver a normalised score between -1 and 1
3. Check if this score becomes the new MAX. If so, save the index
4. Next link...
Begin scraping the page with the highest score (MAX)... repeat...

NOTE: This is NOT a 'find the shortest path' algorithm. This bot will play
the game similar to how a human would. The goal is not to find the shortest 
path at the expense of lots of computing power, and thus time to do so, but 
rather to 'guesstimate' a 'decent enough' path in a very short period of time
by means of some approximation heuristic (e.g. comparing diff between curr
and target URLs).
It may not be the shortest path, but as long as it's efficient and we get
there quick, that's all that matters.